# Article 1: Intelligent Autonomy for Aerospace Engineering Systems
Aerospace engineering systems have undergone technological innovation improvements since first aircraft flights. This inexorable evolution of the aviation industry involves upgrades and updates on Communication, Navigation, and Surveillance (CNS) systems of aircraft as well as Guidance, Navigation, and Control (GNC) systems for air vehicles.

For both, CNS and GNC innovations, the aeronautical community is looking for more intelligent aerospace systems where autonomy plays a key role to keep air transportation safe and secure.

Unmanned Aerial Systems (UASs) are already part of the aviation community, e.g. an increasing number of autonomous UAVs (some of them are experimental but also serious approaches). However, the incorporation of intelligence to UAS decision-making processes to, for instance, either navigate in the case of UAVs, or control the air traffic is still at very early stages.

The analysis and design of IA for UASs require a strong interaction between computer science and engineering. Intelligent Autonomy (IA) solutions for UASs move away from classical engineering approaches to develop systems to now incorporate more specialized computation. However, systems engineering principles will be still applied to simplify the development and management processes of such systems.

Verification is one of the most challenging issue for IA no matter the domain unmanned systems are coming from. The problem is that a truly intelligent and autonomous system can have too many combinations of different output(s) for the same input(s). This makes very difficult the generation of all the possible combinations for the above inputs and outputs. Additionally, it can have the different outputs for the same set of inputs which makes IA behavior tough to predict. Thus, UAS are rather unpredictable, and even worse they become non-determinist; a definitely unwanted quality property for safety, in particular in aviation. A promising approach that researchers propose to use to verify IA is formal methods such as model checking. However, the application of this verification method reduces the solution space since it limits the above input-output relation (combination).

Validation of IAs is reasonably meaningless as verification gets complicated and cannot be done in full. It also suffers the same limitations verification has. This raises concerns about how reliable and robust IA systems can be which ultimately has an impact on the certification of IAs. The challenge set by the verification, validation, and certification of IA for UASs creates a good opportunity to develop innovative methodologies or tools.

# Article 2: Verification of Autonomous Systems for Space Applications
Autonomous software, especially if it is based on model, can play an important role in future space applications. For example, it can help streamline ground operations, or, assist in autonomous rendezvous and docking operations, or even, help recover from problems. Planners can be used to explore the space of recovery actions for a power subsystem and implement a solution without (or with minimal) human intervention. In general, the exploration capabilities of model-based systems give them great flexibility. Unfortunately, it also makes them unpredictable to our human eyes, both in terms of their execution and their verification. The traditional verification techniques are inadequate for these systems since they are mostly based on testing, which implies a very limited exploration of their behavioral space.

## Autonomous System Overview
The system under consideration follows a three-layered architecture.
1. The top layer, also called the decision layer, consists of a domain model and a search engine.
    - The domain model lists the constraints that describe the relationships between elements of the system (which may include environmental constraints) and the flight rules that need to be followed during planning.
    - The search engine does the actual planning, i.e., it elaborates and scheduled activities to meet a given goal under the constraints described in the domain model.

    The output of the decision layer is a plan.

2. The middle layer, also called the executive, takes a plan and “executes” it by issuing commands to the controllers in the bottom layer, also called functional layer. The executive is responsible for issuing commands, checking that commands are actually executed, and responding to exception signals generated by the controllers. 
3. The functional layer is a collection of controllers that actually command the hardware devices of the physical system. This is the lowest level of an autonomy software system.

![autonomous-system-architecture](include/autonomous-system-architecture.png)

In order to have a robust architecture, every component in the autonomous system built with the architecture must be verified.

The architecture also needs to support reconfiguration, not only in terms of adding, removing, or changing components in the functional layer, but also in terms of adding planning and execution capabilities. This guarantees us that the system can be deployed in a variety of domains and that it can evolve with technology progress. Taking into account reconfiguration actually helps the verification process.

## Verification Framework
From a Verification and validation (V&V) point of view, each element requires different techniques, even though some basic techniques (such as code analysis) can be applied across the board.

The functional layer, containing the control software, is the closest to traditional embedded software systems. It mostly consists of control device drivers for actuators or sensors as well as mathematical and algorithmic libraries. For these elements, we are mostly concerned with certifying that they cannot crash (regardless of their inputs) and assessing their performance in terms of execution time and memory management. This suggests that a V&V approach based on embedded system verification can be applied in a straightforward manner.

In some high-level view, the executive and the planner are quite similar. Using a formal model (the domain model for the planner and a plan for the executive), a reasoning engine computes an output (a plan for the planner and low-level commands for the executive). The reasoning engine is usually fairly mechanical (mostly performing searches) and rarely changed (in theory, only the domain model changes for a new application). Therefore, we can afford to bring to bear some “heavy” V&V techniques (e.g., precise static analysis, theorem proving) to verify that the mechanical operation of the reasoning engine (e.g., graph expansion, constraint propagations, and rollbacks) are properly performed.

For the domain model, the concern is well-formedness (are constraints legal?), consistency (can constraints lead to inconsistent solutions or a deadlock?), and completeness (have we specified all necessary constraints?).

![fault-tree](include/fault-tree.png)

As mentioned above, the engine requires checking for “mechanical” properties while models require some functional correctness verification. As shown in the picture, mechanical properties include programming errors (e.g., null pointer de-references), system-level errors (e.g., deadlocks and data races), and data manipulation errors (e.g., plan manipulation and constraint propagation). These properties are organized in a hierarchy that reflects the masking power of each fault class. For example, programming errors might mask synchronization errors, which in turn might mask data manipulation errors. This imposes a natural progression in the verification process: first, eliminate programming errors, then system-level errors, and finally, data manipulation errors. Once we have achieved a satisfying level of certification of “mechanical” properties, we can study performance of the engine (e.g., execution times and memory usage), which is important for on-board execution. Note that mechanical properties are checked only when the engine changes while functional properties are checked for each application (i.e., each new model).

## Reconfiguration
The figure below illustrates the possibilities for reconfiguration in our architecture. First, the most obvious place for reconfiguration is the functional layer where the control software sits. For example, adding a camera to a rover will trigger a reconfiguration in which control software for the camera needs to be added to the functional layer. Generally, such changes trigger changes in both the executive and the planner. The domain model needs to be updated with the constraints specific to the new camera. We can also imagine that new reasoning capabilities (e.g., geometric reasoning) can be added to the planner.

![reconfiguration-of-autonomous-system](include/reconfiguration-of-autonomous-system.png)

Reconfiguring autonomous systems is a complicated 
exercise. If the architecture is not adapted, it can trigger massive changes and result in high cost for the project. Therefore, the architecture needs to be designed for reconfiguration, which means that all aspects of the design need to take into account the possibility of reconfiguration. This is what we have done for verification; we have placed compositional verification at the heart of our verification process. Composition helps re-use verification artifacts in connection with new verification artifacts (for the added components) and efficiently put together the verification of the reconfigured system. In so doing we created a scalable verification mechanism which can deal with reconfiguration.

# Article 5: CyberSecurity for Aerospace Autonomous Systems